Theme: Cognitive augmentation
As data processing and data analytics become more accessible, jobs that can be automated will go away. But to be clear there are still many tasks where the combination of humans and machines produce superior results

In most companies, business users outnumber data scientists and data engineers.  Thus a growing number of tools put the power of advanced analytics in the hands of non-experts (“democratization of data science”). This is driven by demand: many organizations understand the value and consequences of spreading access to data and analytics to many more users. These solutions combine sophisticated algorithms, rich data sets, and intuitive interfaces. This three elements, along with the rise of parallel/distributed computing systems, open up techniques previously confined to experienced data scientists. 

The impact of providing analysts, domain experts, and consumers, with intuitive data science tools that scale to large data sets can be profound. 

	•	Narrative Recommendations: Startup Narrative Science adds descriptive summaries to the output generated by business intelligence tools (dashboards, charts, and tables).  The company uses structured data to discover and extract “facts”, and weaves them together with narrative arcs. The end result is an interface that includes tables, visualizations, and narrative recommendations. 
	•	Visual exploration: Visualizations lie at the heart of some new analytic applications. Companies like Quid and Ayasdi use sophisticated algorithms to uncover patterns and relationships that users navigate through interactive visualizations. A new startup called Graphistry uses GPUs and standard programming tools that let developers build interactive visualizations that scale to millions of points. 
	•	Business Intelligence and Analytics: The success of Qlik and Tableau have spawned companies that scale interactive visual analysis to medium and large data sets. These include companies that target data in Hadoop and other data stores (Datameer, Platfora, Looker).  Palantir and Splunk are large scale data analysis and visualization software that began by targeting users in specific domains (also see Charles River Analytics). Both provide a variety of tools including search, analytics, and reporting.  Another set of companies make it possible for business users to build machine learning pipelines (e.g., Alteryx, Alpine Data, Dataiku). 
	•	New data types: The growth in unstructured data has been well-documented. Most companies are starting to mine documents, tweets and comments, and other sources of unstructured text. In the security domain, video synopsis software like BriefCam allow analysts to wade through hours of video footage in minutes. “Moving dots” (e.g. tracking data from athletics) are being analyzed by companies that specialize in spatio-temporal pattern recognition. Startup Second Spectrum provides analytics to coaches and front offices in many professional basketball teams.  
	•	Consumer Applications: Second Spectrum hopes to launch a consumer product for fans in the near future, at which time it expects its software will be available in tablets for coaches to use in “live game” situations. Waze uses crowdsourced data and algorithms to suggest “best” routes in real time. StitchFix blends domain experts (stylists) and proprietary algorithms in their well-regarded personalized shopping platform. 

Theme: Intelligence Matters
AI      
True AI has been “just around the corner” for 60 years, so why should O’Reilly start covering AI in a big way now? As computing power catches up to scientific and engineering ambitions, and as our ability to learn directly from sensory signals — i.e., big data — increases, intelligent systems are having a real and widespread impact. Every Internet user benefits from these systems today — they sort our email, plan our journeys, answer our questions, and protect us from fraudsters. And, with the Internet of Things, these system have already started to keep our houses and offices comfortable and well-lit, our data centers running more efficiently, our industrial processes humming, and even are driving our cars.
    AI software typology
	•	Software that mimics the brain: the Blue Brain project and Numenta 
	•	Software that doesn’t mimic the brain, but excels in solving practical problems:  
	◦	Q & A systems such as IBM Watson 
	◦	Big Data systems such as Google Brain (Deep Learning) 
	◦	Androids (disembodied voices) such as Softbank’s Pepper 


Algorithms
The “unreasonable effectiveness of data” notwithstanding, algorithms remain an important area of innovation.  Besides the accuracy of an algorithm, companies increasingly need to weigh other considerations such as scalability, speed, and interpretability.



	•	Emerging Algorithms: Deep neural networks (“deep learning”) excel at perception tasks and have been deployed in high-profile consumer products. Google Brain and related efforts inside Baidu, Microsoft, and Facebook are the most prominent examples. Gradient Boosted algorithms have recently become part of many analytic libraries and are popular among data scientists who take part in machine learning competitions.  
	•	Automating (specific) machine learning pipelines: While researchers from Microsoft have made recent progress, deep learning remains hard to parallelize across clusters. A group of researchers from UC Berkeley’s AMPLab made the observation that deep learning automates a series of tasks (including feature engineering) into a pipeline. They built a series of components on top of Apache Spark that mimics this pipeline, using transparent pieces that scale across clusters. The resulting system cuts down “training” time substantially and achieves the same performance numbers as deep learning on computer vision benchmarks. 
	•	SaaS: Through a set of predictive APIs, services like prediction.io, BigML, Cognitive Scale, and Google, make it easy for organizations to use big data analytics in their decision making. 
	•	Feature Engineering: Many data scientists prefer simple models that are easy to explain and that lead to actionable insights. A well-known adage in machine learning states that “good features allow a simple model to beat a complex model”. To that end, the search for features (predictive variables) is an an active area for both researchers and startups. Users of SparkBeyond are able to incorporate the company’s knowledge databases (Wikipedia, OpenStreetMap, Github, etc.) to enrich their own data and incorporate interpretable features into analytic models. Startup ContextRelevant has a similar service for automating feature engineering and discovery. Financial services companies like BillGuard (location data), ZestFinance & Lenddo (social media) have developed analytic tools that leverage non-traditional data sources. 




	•	Unstructured data: The growth in unstructured data has been well-documented. Techniques like deep learning started garnering attention because of successful applications in images, audio, video, and text (machine translation).  
	•	Active Learning (“human-in-the-loop” machine learning):  A popular technique called active learning uses algorithms to handle a majority of cases, and routes only the difficult cases to humans. Locu built an extremely accurate active learning framework for extracting structured data from web pages. SpeakerText uses speech-to-text software supplemented with Active Learning, for its transcription service. CrowdFlower has a web service that simplifies the Active Learning workflow. 
	•	Secure Machine Learning: As intelligent systems become common, security and privacy become critical. One area we’re particularly interested in is adversarial analytics: the use of algorithms to attack or undermine the effectiveness of (intelligent) systems. To that end there is growing interest (in the academic literature: see [1], [2]) in machine learning systems that are immune to resourceful adversaries.  
	•	Interface languages: R and Python are popular among data scientists in both academia and industry. Over the past year, libraries of scalable algorithms that target R and Python users have been released as part of Adatao, H20, GraphLab, and Spark. These libraries take R or Python code and execute them in distributed frameworks like Spark or H20.  

 


Theme: The convergence of cheap sensors, fast networks, and distributed computation
IT operations groups have had tools for storing, mining, and visualizing time-series and event data. The emergence of large data centers have led to new solutions capable of handling much bigger volumes of data. These new distributed systems for storing, managing, and mining event data are based on popular (open source) components in the big data ecosystem.

As sensors get deployed to everyday physical objects and devices (wearables, mobile phones), data collected by sensors are being analyzed using similar tools and techniques.  The combination of streaming data (from sensors) with static data lies at the heart of many data applications in consumer marketing and advertising, preventive maintenance, smart automation, and health monitoring applications. 



Both “sensor data” and “internet of things” were trending phrases among proposals submitted to Strata this year. The renewed interest in event data coincides with ordinary physical objects being placed online. When intelligent objects are connected they can seamlessly work together and start assisting users in meaningful ways.

Current analytic solutions that target IoT place computation mainly in centralized servers or cloud services. Bandwidth constraints, and the need for resilience and real-time response rates, points towards more computation being pushed closer to “the edge”.

	•	Event data:  Associated with machine generated logs, Event data is comprised of an “action or metric”, a timestamp, and associated “entities” (username, location, etc). Systems for dealing with event data address two general areas: (1) the mechanics of collecting, storing, managing data, and (2) analytics for extracting actionable information. 
	•	Scaleout systems from I.T. operations: Collecting, storing, mining large amounts of event data is an active area of research. Many current systems were designed to help I.T. administrators manage large number of servers and software systems. These include proprietary solutions like Splunk, SumoLogic, CloudPhysics, and Twitter’s Observability stack. Open source solutions include OpenTSDB, Graphite, and the ELK stack (ElasticSearch + Logstash + Kibana).  
	•	Huge volumes of time series: Monitoring, storing, and analyzing large numbers of time series lies at the heart of many modern systems (Twitter’s Observability stack is used to monitor hundreds of millions of time series). At the most basic level, systems let users track a few time series through charts and dashboards. Search, anomaly detection, correlation, and root cause analysis can be found in varying degrees in systems that target I.T. operations. CloudPhysics uses crowdsourced data to build simulators that allow system administrators to perform “what-if” analysis prior to rolling out upgrades or patches. 
	•	Data analysis on streams: Analyzing continuously arriving data (“at scale”) can be challenging. Since metrics computed from streaming data are constantly changing, in most use cases approximate answers suffice. Approximate counting techniques such as hyperloglog and sketches are being built on top of on top of distributed streaming systems like Summingbird, Storm, and Spark Streaming. 
	•	Applications: The applications of analytics on event data include personal health, (home/building) automation, logistics, manufacturing, and the design of “smart cities”. Using data from their activity tracking system, Jawbone is analyzing the sleep and exercise patterns of millions of users (in contrast, traditional “sleep studies” are based on very small samples). Telefonica Smart Steps and Path Intelligence use signals from mobile devices to measure and predict consumer behavior in physical spaces. GridSense and C3 have analytic solutions for “smart electric grids”. The use of big data analytics to make sense of data from connected devices is an area where enterprise software vendors including GE (“Industrial Internet”) and Pivotal (“smart cities”) are actively building solutions.  

  


Theme: Reproducing, managing, and maintaining Data pipelines
The “mythical” data scientist is someone adept at handling all facets of a data project. The reality is quite different: in many companies there are data engineers who specialize in acquiring and storing data as well as building and maintaining data infrastructures. Data scientists usually refer to specialists in analytics, while visual designers are those responsible for building interactive visual applications.



Analytic projects involve a series of steps that often require many different tools. There are a growing number of companies and open source projects that integrate many analytic tools into coherent user interfaces and packages. Many of these integrated tools enable replication, collaboration, and deployment. This remains an active area as specialized tools rush to broaden their coverage of analytic pipelines.

	•	Challenges: Decrease in productivity due to frequent “context-switching”, reproducibility, and ease of collaboration while working on data projects. For data pipelines that need to be deployed in production, maintenance and ongoing assessment (monitoring) are additional issues that need to be addressed. 
	•	Towards an integrated suite of tools: Data pipelines may involve a variety of tools and programming languages. Users who want to stick with a single programming language can take solace in the fact that the R and Python data (“pydata”) communities are addressing this problem. Among distributed computing frameworks, Apache Spark provides a unified programming interface for a variety of data engineering and data science tasks. The major Hadoop vendors have announced initiatives to use Spark as their primary processing engine. 



	•	Workflow tools: Directed acyclic graphs are a popular interface for building and maintaining data pipelines. These tools target different types of users: business analysts (Alpine Data, Alteryx, Dataiku), data scientists (GraphLab Canvas), and data engineers (Driven from Cascading, chronos from airbnb, Azkaban). 
	•	Notebooks: An interface pioneered by Mathematica, notebooks let users combine code, graphics, and text. The emergence of IPython notebooks have led to renewed interest in notebooks as tools for managing data projects (notebooks remain popular delivery tools for instructional material). IPython notebooks have gone beyond the Python programming language and the underlying technology is being used by other language communities. Beaker Notebooks let data scientists create pipelines using several programming and markup languages. A new notebook interface that targets both data scientists and data engineers, seamlessly ties together the Apache Spark stack. Databricks workspace lets users build notebooks that use different components of the Spark stack (notebooks can be linked together in a pipeline).  

	•	Workbooks: As they begin to take on more data tasks, business analysts are beginning to have access to tools that let them place (visual/data) analysis, data import and wrangling steps, into reproducible “workbooks”. These workbooks can then be viewed and copied by others, and also serve as a place where many users can collaborate. Datameer, ClearStory, Trifacta, and Platfora are examples of tools that support this concept.  
	•	Monitoring and scheduling: As they become popular interfaces for building data pipelines, workflow tools and notebooks are starting to add monitoring and scheduling features.  



Theme: Evolving, maturing marketplace of Big Data components
Many popular components in the big data ecosystem are open source. As such many companies build their data infrastructure and products by assembling components like Spark, Kafka, Cassandra, ElasticSearch, among others. Contrast that to a few years ago when many of these components weren’t ready (or didn’t exist) and companies built similar technologies from scratch.

But companies are interested in solutions, not in software components. At their core big data applications merge large amounts of real-time and static data to improve decision-making:

 

This simple idea can be hard to execute in practice (think volume, variety, velocity). Unlocking value from disparate data sources entails some familiarity with domain-specific data sources, requirements, and business problems. So at least among startups we’re tracking, those that focus on and optimize for specific verticals (industries) seem to fare better.

Experienced data engineers can assemble many big data components quickly. They are key to building data infrastructures. So while the popular press and universities focus on data scientists, it turns out skilled data engineers are even harder to hire.

	•	Distributed computing research: Advances in systems and data management research have led to a proliferation of tools from academia and industry (as well as interest in concepts like the CAP Theorem and distributed consensus algorithms). The most popular big data framework Apache Spark, was originally a research project at UC Berkeley’s AMPLab: 


[the Berkeley Data Analytics stack]

	•	Data platforms: With the emergence of Apache Spark as an alternative to Hadoop MapReduce, the Berkeley Data Analytics stack (BDAS) provides a glimpse into modern data platforms:  



	•	Big Data components: Data engineers have access to components that they can assemble into data platforms and applications. Many of these components are open source, and quite a few are in the Java ecosystem. More importantly many these components boast of numerous production deployments, extensive developer and user communities, and improving documentation. 
	•	Storage: Of all big data software components, distributed data management systems fueled the rise of big data applications. They range from offline (HDFS, data warehouse), to online (nosql databases), and asynchronous systems (Kafka, stream processing systems). 
	•	Data integration: Maintaining robust data flows is a central task of data engineers. Modern data applications draw data from many different source systems, and involve different workloads and latency requirements. In a forthcoming O’Reilly book, Jay Kreps describes an architecture for data integration that evolved out of his experiences at Linkedin. 
	•	(Vertical) Big Data applications: Not only are the essential software components open source, there is now a cohort of data engineers adept at assembling them into applications. Recently a team from AMPLab used Spark and a few other components to build a system  that dramatically speeds up an important genomics data pipeline used in cancer research. Delve into the details of a successful big data solution like the one AMPLab built for genomics, and you realize that robust data integration and actionable analytics require domain knowledge. Neuroscience researchers have developed libraries on Apache Spark for analyzing the large amounts of time-series data that arise from their controlled experiments.  Israel startup Treato merges data from online sources with health databases, and applies Natural Language tools tuned to extract health experiences from the web. The result is a platform that measures patients’ concerns and problems (the company aspires to be the “voice of patients”). Startup Guavus specializes in big data solutions for the telecom industry (specifically mobile carriers). RocketFuel builds advanced analytic solutions for advertising and marketing. Treato, Guavus and RocketFuel continue to build big data applications using the open source components that have come of age over the last few years. 



Theme: The value of applying techniques from Design and Social Science
To be clear data scientists have always drawn from social science (e.g., surveys, psychometrics) and design. But we are noticing that many more data scientists are expanding their collaboration with product designers and social scientists. Some data scientists are incorporating techniques like behavioral economics, ethnography, and ideation workshops, others are learning soft skills including
	•	problem scoping 
	•	asking the right questions 
	•	constructing narrative arcs and story-telling 
	•	formulating arguments 
	•	causal inference  (also see this) 
Members of IDEO’s Hybrid Insights group use the following techniques for enriching quantitative with qualitative insights:
	•	Embed stories into data to enable decision makers and strategists to connect with and remember important quantitative metrics and statistics.  
	•	Cross-validate insights by taking qualitative observations from initial studies, and validating them with large-scale quantitative research. 
	•	Enable users to see the value of new concepts by placing disruptive innovations in contexts users can understand. 



Theme: The importance of building a Data Culture
Data-driven / Deciding Better: “Data-driven” organizations excel at using data to improve decision making. It all starts with instrumentation - as O’Reilly author DJ Patil points out “if you can’t measure it, you can’t fix it”. So to a large extent, this is an IT challenge (usually involving data integration). This implies a commitment to ensuring data is well-documented and error free. Data-driven organizations also foster a culture of curiosity and actively encourage members to ask questions about data/reports they are consuming. For analytics, this means encouraging data scientists to develop models and tools that lead to actionable insights (rather than “black boxes”).

Data Products: Developments in distributed computing over the past decade have given rise to a group of (mostly technology) companies that excel in putting their data to use by building data products. Sites like Linkedin (“people you may know”), Twitter (“trending topics”, “who to follow”) , Facebook (“news feed”), and Google (“Google Trends”), and others build engaging features using data collected from users. In many instances, data products evolve in stages (starting with a “minimum viable product”) and are built by cross-functional teams that embrace alternative analysis techniques.

Culture: With data engineers and data scientists in high demand, we continue to monitor how companies attract talent and build cultures that allow them to flourish. Skilled talent want to be able to communicate with their peers in other companies. Thus thought leadership and transparency turn out to be good recruiting tools. To that end, many more companies encourage employees to write blog posts and contribute to open source projects. 

Resources:
	•	Building Data Science Teams: Strata report by DJ Patil 
	•	Data Jujitsu, The Art of Turning Data Into Product: Strata report by DJ Patil 
	•	Just Enough Math video 

Resources
	•	Alistair Croll 
	•	James Cham and Shivon Zilis, Bloomberg Beta 
	•	Joe Adler, Interana 
	•	Abdur Chowdury, ex-Twitter 

Theme: Perils of Big Data
Poor analysis: 
	•	Correlations: Every few months, there seems to be an article criticizing the hype surrounding big data. Most of the criticisms point to poor analysis and highlight issues known to experienced data analysts. Given that big data analysis involves correlations, old issues in correlation analysis tend to crop up (correlation is not causation, correlations can change over time, and data dredging). Along these lines Cosma Shalizi of CMU has been a proponent of the use of machine learning techniques in economics.  
	•	Domain expertise: Does big data eliminate the need for domain knowledge? There are examples that downplay the importance of domain expertise, but we know many data scientists who disagree with this assessment.  

Law, ethics, and society: 
	•	Privacy: Issues of privacy and surveillance usually involve algorithms operating against users data. Strata keynote speaker Julia Angwin has written a book documenting the “surveillance economy”. Foo campers Ed Felten and Arvind Narayanan have written extensively on data anonymization.  
	•	Cultural impact of models: Another interesting trend is the small but growing number of data scientists calling for studies that quantify the cultural impact of models. A widely cited example is the algorithmic pricing: varying price based on the location of users. An upcoming conference is devoted to this topic: Fairness, Accountability, and Transparency in Machine Learning (Researchers address “... growing anxieties about the role that machine learning plays in consequential decision-making in such areas as commerce, employment, health care, education, and policing.”) Stanford University recently announced an initiative to study the long-term implications of Artificial Intelligence on “all aspects of life”. 
	•	Work on stuff that matters: Organizations and programs like Code for America, Bayes Impact, Datakind, and the Presidential Innovation Fellows match municipalities and government agencies, with experienced data professionals. 

